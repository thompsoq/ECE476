digraph {
	graph [size="63.15,63.15"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1956752866656 [label="
 (32, 10)" fillcolor=darkolivegreen1]
	1956752602976 [label=SoftmaxBackward0]
	1956752601632 -> 1956752602976
	1956752601632 [label=AddmmBackward0]
	1956752602928 -> 1956752601632
	1956461620336 [label="outlinear.bias
 (10)" fillcolor=lightblue]
	1956461620336 -> 1956752602928
	1956752602928 [label=AccumulateGrad]
	1956752604272 -> 1956752601632
	1956752604272 [label=FusedDropoutBackward0]
	1956752630976 -> 1956752604272
	1956752630976 [label=LeakyReluBackward0]
	1956752631600 -> 1956752630976
	1956752631600 [label=AddmmBackward0]
	1956752631696 -> 1956752631600
	1956461619376 [label="bottleneck.bias
 (10)" fillcolor=lightblue]
	1956461619376 -> 1956752631696
	1956752631696 [label=AccumulateGrad]
	1956752631648 -> 1956752631600
	1956752631648 [label=ReshapeAliasBackward0]
	1956752631888 -> 1956752631648
	1956752631888 [label=AvgPool2DBackward0]
	1956752632080 -> 1956752631888
	1956752632080 [label=LeakyReluBackward0]
	1956752632176 -> 1956752632080
	1956752632176 [label=CudnnBatchNormBackward0]
	1956752632272 -> 1956752632176
	1956752632272 [label=AddBackward0]
	1956752632464 -> 1956752632272
	1956752632464 [label=CudnnConvolutionBackward0]
	1956752632560 -> 1956752632464
	1956752632560 [label=MaxPool2DWithIndicesBackward0]
	1956752632752 -> 1956752632560
	1956752632752 [label=AddBackward0]
	1956752632800 -> 1956752632752
	1956752632800 [label=AddBackward0]
	1956752633040 -> 1956752632800
	1956752633040 [label=LeakyReluBackward0]
	1956752633184 -> 1956752633040
	1956752633184 [label=CudnnBatchNormBackward0]
	1956752633232 -> 1956752633184
	1956752633232 [label=AddBackward0]
	1956752633520 -> 1956752633232
	1956752633520 [label=CudnnConvolutionBackward0]
	1956752633664 -> 1956752633520
	1956752633664 [label=MaxPool2DWithIndicesBackward0]
	1956752633808 -> 1956752633664
	1956752633808 [label=AddBackward0]
	1956752707696 -> 1956752633808
	1956752707696 [label=AddBackward0]
	1956752707888 -> 1956752707696
	1956752707888 [label=LeakyReluBackward0]
	1956752708032 -> 1956752707888
	1956752708032 [label=CudnnBatchNormBackward0]
	1956752708080 -> 1956752708032
	1956752708080 [label=AddBackward0]
	1956752708368 -> 1956752708080
	1956752708368 [label=CudnnConvolutionBackward0]
	1956752708512 -> 1956752708368
	1956752708512 [label=MaxPool2DWithIndicesBackward0]
	1956752708656 -> 1956752708512
	1956752708656 [label=LeakyReluBackward0]
	1956752708704 -> 1956752708656
	1956752708704 [label=CudnnBatchNormBackward0]
	1956752708848 -> 1956752708704
	1956752708848 [label=AddBackward0]
	1956752709136 -> 1956752708848
	1956752709136 [label=CudnnConvolutionBackward0]
	1956752709280 -> 1956752709136
	1956752709280 [label=MaxPool2DWithIndicesBackward0]
	1956752709424 -> 1956752709280
	1956752709424 [label=LeakyReluBackward0]
	1956752709472 -> 1956752709424
	1956752709472 [label=CudnnBatchNormBackward0]
	1956752709616 -> 1956752709472
	1956752709616 [label=AddBackward0]
	1956752709904 -> 1956752709616
	1956752709904 [label=CudnnConvolutionBackward0]
	1956752710048 -> 1956752709904
	1956752710048 [label=MaxPool2DWithIndicesBackward0]
	1956752710192 -> 1956752710048
	1956752710192 [label=LeakyReluBackward0]
	1956752710240 -> 1956752710192
	1956752710240 [label=CudnnBatchNormBackward0]
	1956752710384 -> 1956752710240
	1956752710384 [label=AddBackward0]
	1956752710672 -> 1956752710384
	1956752710672 [label=CudnnConvolutionBackward0]
	1956752710816 -> 1956752710672
	1956461674800 [label="conv.entry_LoRa_1.weight
 (4, 1, 1, 7)" fillcolor=lightblue]
	1956461674800 -> 1956752710816
	1956752710816 [label=AccumulateGrad]
	1956752710624 -> 1956752710384
	1956752710624 [label=ReshapeAliasBackward0]
	1956752711008 -> 1956752710624
	1956461674880 [label="conv.entry_LoRa_1.bias
 (4)" fillcolor=lightblue]
	1956461674880 -> 1956752711008
	1956752711008 [label=AccumulateGrad]
	1956752710336 -> 1956752710240
	1956461675200 [label="conv_bn.entry_LoRa_1.weight
 (4)" fillcolor=lightblue]
	1956461675200 -> 1956752710336
	1956752710336 [label=AccumulateGrad]
	1956752710480 -> 1956752710240
	1956461675280 [label="conv_bn.entry_LoRa_1.bias
 (4)" fillcolor=lightblue]
	1956461675280 -> 1956752710480
	1956752710480 [label=AccumulateGrad]
	1956752710000 -> 1956752709904
	1956461676400 [label="conv.entry_LoRa_2.weight
 (8, 4, 1, 5)" fillcolor=lightblue]
	1956461676400 -> 1956752710000
	1956752710000 [label=AccumulateGrad]
	1956752709856 -> 1956752709616
	1956752709856 [label=ReshapeAliasBackward0]
	1956752710096 -> 1956752709856
	1956461676480 [label="conv.entry_LoRa_2.bias
 (8)" fillcolor=lightblue]
	1956461676480 -> 1956752710096
	1956752710096 [label=AccumulateGrad]
	1956752709568 -> 1956752709472
	1956461676560 [label="conv_bn.entry_LoRa_2.weight
 (8)" fillcolor=lightblue]
	1956461676560 -> 1956752709568
	1956752709568 [label=AccumulateGrad]
	1956752709712 -> 1956752709472
	1956461676640 [label="conv_bn.entry_LoRa_2.bias
 (8)" fillcolor=lightblue]
	1956461676640 -> 1956752709712
	1956752709712 [label=AccumulateGrad]
	1956752709232 -> 1956752709136
	1956461677040 [label="conv.entry_LoRa_3.weight
 (12, 8, 1, 3)" fillcolor=lightblue]
	1956461677040 -> 1956752709232
	1956752709232 [label=AccumulateGrad]
	1956752709088 -> 1956752708848
	1956752709088 [label=ReshapeAliasBackward0]
	1956752709328 -> 1956752709088
	1956461677120 [label="conv.entry_LoRa_3.bias
 (12)" fillcolor=lightblue]
	1956461677120 -> 1956752709328
	1956752709328 [label=AccumulateGrad]
	1956752708800 -> 1956752708704
	1956461677200 [label="conv_bn.entry_LoRa_3.weight
 (12)" fillcolor=lightblue]
	1956461677200 -> 1956752708800
	1956752708800 [label=AccumulateGrad]
	1956752708944 -> 1956752708704
	1956461677280 [label="conv_bn.entry_LoRa_3.bias
 (12)" fillcolor=lightblue]
	1956461677280 -> 1956752708944
	1956752708944 [label=AccumulateGrad]
	1956752708464 -> 1956752708368
	1956461677680 [label="conv.res_LoRa_4exp.weight
 (24, 12, 1, 1)" fillcolor=lightblue]
	1956461677680 -> 1956752708464
	1956752708464 [label=AccumulateGrad]
	1956752708320 -> 1956752708080
	1956752708320 [label=ReshapeAliasBackward0]
	1956752708560 -> 1956752708320
	1956461677760 [label="conv.res_LoRa_4exp.bias
 (24)" fillcolor=lightblue]
	1956461677760 -> 1956752708560
	1956752708560 [label=AccumulateGrad]
	1956752707936 -> 1956752708032
	1956461677840 [label="conv_bn.res_LoRa_4exp.weight
 (24)" fillcolor=lightblue]
	1956461677840 -> 1956752707936
	1956752707936 [label=AccumulateGrad]
	1956752708176 -> 1956752708032
	1956461677920 [label="conv_bn.res_LoRa_4exp.bias
 (24)" fillcolor=lightblue]
	1956461677920 -> 1956752708176
	1956752708176 [label=AccumulateGrad]
	1956752707840 -> 1956752707696
	1956752707840 [label=LeakyReluBackward0]
	1956752708272 -> 1956752707840
	1956752708272 [label=CudnnBatchNormBackward0]
	1956752709040 -> 1956752708272
	1956752709040 [label=AddBackward0]
	1956752709808 -> 1956752709040
	1956752709808 [label=CudnnConvolutionBackward0]
	1956752710528 -> 1956752709808
	1956752710528 [label=LeakyReluBackward0]
	1956752709952 -> 1956752710528
	1956752709952 [label=CudnnBatchNormBackward0]
	1956752711152 -> 1956752709952
	1956752711152 [label=AddBackward0]
	1956752711248 -> 1956752711152
	1956752711248 [label=CudnnConvolutionBackward0]
	1956752708512 -> 1956752711248
	1956752711392 -> 1956752711248
	1956461678400 [label="conv.res_LoRa_41.weight
 (24, 12, 1, 3)" fillcolor=lightblue]
	1956461678400 -> 1956752711392
	1956752711392 [label=AccumulateGrad]
	1956752711200 -> 1956752711152
	1956752711200 [label=ReshapeAliasBackward0]
	1956752711440 -> 1956752711200
	1956461678480 [label="conv.res_LoRa_41.bias
 (24)" fillcolor=lightblue]
	1956461678480 -> 1956752711440
	1956752711440 [label=AccumulateGrad]
	1956752711056 -> 1956752709952
	1956461719616 [label="conv_bn.res_LoRa_41.weight
 (24)" fillcolor=lightblue]
	1956461719616 -> 1956752711056
	1956752711056 [label=AccumulateGrad]
	1956752710576 -> 1956752709952
	1956461719696 [label="conv_bn.res_LoRa_41.bias
 (24)" fillcolor=lightblue]
	1956461719696 -> 1956752710576
	1956752710576 [label=AccumulateGrad]
	1956752709184 -> 1956752709808
	1956461720096 [label="conv.res_LoRa_42.weight
 (24, 24, 1, 3)" fillcolor=lightblue]
	1956461720096 -> 1956752709184
	1956752709184 [label=AccumulateGrad]
	1956752709760 -> 1956752709040
	1956752709760 [label=ReshapeAliasBackward0]
	1956752711104 -> 1956752709760
	1956461720176 [label="conv.res_LoRa_42.bias
 (24)" fillcolor=lightblue]
	1956461720176 -> 1956752711104
	1956752711104 [label=AccumulateGrad]
	1956752708992 -> 1956752708272
	1956461720256 [label="conv_bn.res_LoRa_42.weight
 (24)" fillcolor=lightblue]
	1956461720256 -> 1956752708992
	1956752708992 [label=AccumulateGrad]
	1956752707984 -> 1956752708272
	1956461720336 [label="conv_bn.res_LoRa_42.bias
 (24)" fillcolor=lightblue]
	1956461720336 -> 1956752707984
	1956752707984 [label=AccumulateGrad]
	1956752707648 -> 1956752633808
	1956752707648 [label=LeakyReluBackward0]
	1956752708608 -> 1956752707648
	1956752708608 [label=CudnnBatchNormBackward0]
	1956752710960 -> 1956752708608
	1956752710960 [label=AddBackward0]
	1956752711536 -> 1956752710960
	1956752711536 [label=CudnnConvolutionBackward0]
	1956752711584 -> 1956752711536
	1956752711584 [label=LeakyReluBackward0]
	1956752711632 -> 1956752711584
	1956752711632 [label=CudnnBatchNormBackward0]
	1956752728272 -> 1956752711632
	1956752728272 [label=AddBackward0]
	1956752728464 -> 1956752728272
	1956752728464 [label=CudnnConvolutionBackward0]
	1956752707696 -> 1956752728464
	1956752728608 -> 1956752728464
	1956461720736 [label="conv.res_LoRa_51.weight
 (24, 24, 1, 3)" fillcolor=lightblue]
	1956461720736 -> 1956752728608
	1956752728608 [label=AccumulateGrad]
	1956752728416 -> 1956752728272
	1956752728416 [label=ReshapeAliasBackward0]
	1956752728656 -> 1956752728416
	1956461720816 [label="conv.res_LoRa_51.bias
 (24)" fillcolor=lightblue]
	1956461720816 -> 1956752728656
	1956752728656 [label=AccumulateGrad]
	1956752728224 -> 1956752711632
	1956461720896 [label="conv_bn.res_LoRa_51.weight
 (24)" fillcolor=lightblue]
	1956461720896 -> 1956752728224
	1956752728224 [label=AccumulateGrad]
	1956752728128 -> 1956752711632
	1956461720976 [label="conv_bn.res_LoRa_51.bias
 (24)" fillcolor=lightblue]
	1956461720976 -> 1956752728128
	1956752728128 [label=AccumulateGrad]
	1956752711344 -> 1956752711536
	1956461721376 [label="conv.res_LoRa_52.weight
 (24, 24, 1, 3)" fillcolor=lightblue]
	1956461721376 -> 1956752711344
	1956752711344 [label=AccumulateGrad]
	1956752709376 -> 1956752710960
	1956752709376 [label=ReshapeAliasBackward0]
	1956752711296 -> 1956752709376
	1956461721456 [label="conv.res_LoRa_52.bias
 (24)" fillcolor=lightblue]
	1956461721456 -> 1956752711296
	1956752711296 [label=AccumulateGrad]
	1956752708416 -> 1956752708608
	1956461721536 [label="conv_bn.res_LoRa_52.weight
 (24)" fillcolor=lightblue]
	1956461721536 -> 1956752708416
	1956752708416 [label=AccumulateGrad]
	1956752707792 -> 1956752708608
	1956461721616 [label="conv_bn.res_LoRa_52.bias
 (24)" fillcolor=lightblue]
	1956461721616 -> 1956752707792
	1956752707792 [label=AccumulateGrad]
	1956752633616 -> 1956752633520
	1956461722016 [label="conv.res_LoRa_6exp.weight
 (48, 24, 1, 1)" fillcolor=lightblue]
	1956461722016 -> 1956752633616
	1956752633616 [label=AccumulateGrad]
	1956752633472 -> 1956752633232
	1956752633472 [label=ReshapeAliasBackward0]
	1956752633760 -> 1956752633472
	1956461722096 [label="conv.res_LoRa_6exp.bias
 (48)" fillcolor=lightblue]
	1956461722096 -> 1956752633760
	1956752633760 [label=AccumulateGrad]
	1956752633088 -> 1956752633184
	1956461722176 [label="conv_bn.res_LoRa_6exp.weight
 (48)" fillcolor=lightblue]
	1956461722176 -> 1956752633088
	1956752633088 [label=AccumulateGrad]
	1956752633328 -> 1956752633184
	1956461722256 [label="conv_bn.res_LoRa_6exp.bias
 (48)" fillcolor=lightblue]
	1956461722256 -> 1956752633328
	1956752633328 [label=AccumulateGrad]
	1956752632992 -> 1956752632800
	1956752632992 [label=LeakyReluBackward0]
	1956752633424 -> 1956752632992
	1956752633424 [label=CudnnBatchNormBackward0]
	1956752633568 -> 1956752633424
	1956752633568 [label=AddBackward0]
	1956752710144 -> 1956752633568
	1956752710144 [label=CudnnConvolutionBackward0]
	1956752728368 -> 1956752710144
	1956752728368 [label=LeakyReluBackward0]
	1956752728560 -> 1956752728368
	1956752728560 [label=CudnnBatchNormBackward0]
	1956752728848 -> 1956752728560
	1956752728848 [label=AddBackward0]
	1956752729040 -> 1956752728848
	1956752729040 [label=CudnnConvolutionBackward0]
	1956752633664 -> 1956752729040
	1956752729184 -> 1956752729040
	1956461722656 [label="conv.res_LoRa_61.weight
 (48, 24, 1, 3)" fillcolor=lightblue]
	1956461722656 -> 1956752729184
	1956752729184 [label=AccumulateGrad]
	1956752728992 -> 1956752728848
	1956752728992 [label=ReshapeAliasBackward0]
	1956752729232 -> 1956752728992
	1956461722736 [label="conv.res_LoRa_61.bias
 (48)" fillcolor=lightblue]
	1956461722736 -> 1956752729232
	1956752729232 [label=AccumulateGrad]
	1956752728800 -> 1956752728560
	1956461722816 [label="conv_bn.res_LoRa_61.weight
 (48)" fillcolor=lightblue]
	1956461722816 -> 1956752728800
	1956752728800 [label=AccumulateGrad]
	1956752728752 -> 1956752728560
	1956461722896 [label="conv_bn.res_LoRa_61.bias
 (48)" fillcolor=lightblue]
	1956461722896 -> 1956752728752
	1956752728752 [label=AccumulateGrad]
	1956752728176 -> 1956752710144
	1956461723296 [label="conv.res_LoRa_62.weight
 (48, 48, 1, 3)" fillcolor=lightblue]
	1956461723296 -> 1956752728176
	1956752728176 [label=AccumulateGrad]
	1956752708224 -> 1956752633568
	1956752708224 [label=ReshapeAliasBackward0]
	1956752728896 -> 1956752708224
	1956461723376 [label="conv.res_LoRa_62.bias
 (48)" fillcolor=lightblue]
	1956461723376 -> 1956752728896
	1956752728896 [label=AccumulateGrad]
	1956752633136 -> 1956752633424
	1956461723456 [label="conv_bn.res_LoRa_62.weight
 (48)" fillcolor=lightblue]
	1956461723456 -> 1956752633136
	1956752633136 [label=AccumulateGrad]
	1956752711488 -> 1956752633424
	1956461723536 [label="conv_bn.res_LoRa_62.bias
 (48)" fillcolor=lightblue]
	1956461723536 -> 1956752711488
	1956752711488 [label=AccumulateGrad]
	1956752632656 -> 1956752632752
	1956752632656 [label=LeakyReluBackward0]
	1956752707744 -> 1956752632656
	1956752707744 [label=CudnnBatchNormBackward0]
	1956752632944 -> 1956752707744
	1956752632944 [label=AddBackward0]
	1956752729328 -> 1956752632944
	1956752729328 [label=CudnnConvolutionBackward0]
	1956752729376 -> 1956752729328
	1956752729376 [label=LeakyReluBackward0]
	1956752729520 -> 1956752729376
	1956752729520 [label=CudnnBatchNormBackward0]
	1956752729616 -> 1956752729520
	1956752729616 [label=AddBackward0]
	1956752729808 -> 1956752729616
	1956752729808 [label=CudnnConvolutionBackward0]
	1956752632800 -> 1956752729808
	1956752729952 -> 1956752729808
	1956472246656 [label="conv.res_LoRa_71.weight
 (48, 48, 1, 3)" fillcolor=lightblue]
	1956472246656 -> 1956752729952
	1956752729952 [label=AccumulateGrad]
	1956752729760 -> 1956752729616
	1956752729760 [label=ReshapeAliasBackward0]
	1956752730000 -> 1956752729760
	1956472246736 [label="conv.res_LoRa_71.bias
 (48)" fillcolor=lightblue]
	1956472246736 -> 1956752730000
	1956752730000 [label=AccumulateGrad]
	1956752729568 -> 1956752729520
	1956472246816 [label="conv_bn.res_LoRa_71.weight
 (48)" fillcolor=lightblue]
	1956472246816 -> 1956752729568
	1956752729568 [label=AccumulateGrad]
	1956752729424 -> 1956752729520
	1956472246896 [label="conv_bn.res_LoRa_71.bias
 (48)" fillcolor=lightblue]
	1956472246896 -> 1956752729424
	1956752729424 [label=AccumulateGrad]
	1956752729136 -> 1956752729328
	1956472247296 [label="conv.res_LoRa_72.weight
 (48, 48, 1, 3)" fillcolor=lightblue]
	1956472247296 -> 1956752729136
	1956752729136 [label=AccumulateGrad]
	1956752728704 -> 1956752632944
	1956752728704 [label=ReshapeAliasBackward0]
	1956752729664 -> 1956752728704
	1956472247376 [label="conv.res_LoRa_72.bias
 (48)" fillcolor=lightblue]
	1956472247376 -> 1956752729664
	1956752729664 [label=AccumulateGrad]
	1956752728944 -> 1956752707744
	1956472247456 [label="conv_bn.res_LoRa_72.weight
 (48)" fillcolor=lightblue]
	1956472247456 -> 1956752728944
	1956752728944 [label=AccumulateGrad]
	1956752728320 -> 1956752707744
	1956472247536 [label="conv_bn.res_LoRa_72.bias
 (48)" fillcolor=lightblue]
	1956472247536 -> 1956752728320
	1956752728320 [label=AccumulateGrad]
	1956752632608 -> 1956752632464
	1956472247936 [label="conv.distinct_LoRa_8.weight
 (96, 48, 2, 3)" fillcolor=lightblue]
	1956472247936 -> 1956752632608
	1956752632608 [label=AccumulateGrad]
	1956752632416 -> 1956752632272
	1956752632416 [label=ReshapeAliasBackward0]
	1956752632896 -> 1956752632416
	1956472248016 [label="conv.distinct_LoRa_8.bias
 (96)" fillcolor=lightblue]
	1956472248016 -> 1956752632896
	1956752632896 [label=AccumulateGrad]
	1956752632224 -> 1956752632176
	1956472248096 [label="conv_bn.distinct_LoRa_8.weight
 (96)" fillcolor=lightblue]
	1956472248096 -> 1956752632224
	1956752632224 [label=AccumulateGrad]
	1956752631984 -> 1956752632176
	1956472248176 [label="conv_bn.distinct_LoRa_8.bias
 (96)" fillcolor=lightblue]
	1956472248176 -> 1956752631984
	1956752631984 [label=AccumulateGrad]
	1956752631504 -> 1956752631600
	1956752631504 [label=TBackward0]
	1956752632128 -> 1956752631504
	1956461619136 [label="bottleneck.weight
 (10, 96)" fillcolor=lightblue]
	1956461619136 -> 1956752632128
	1956752632128 [label=AccumulateGrad]
	1956752601584 -> 1956752601632
	1956752601584 [label=TBackward0]
	1956752631744 -> 1956752601584
	1956461620256 [label="outlinear.weight
 (10, 10)" fillcolor=lightblue]
	1956461620256 -> 1956752631744
	1956752631744 [label=AccumulateGrad]
	1956752602976 -> 1956752866656
}
